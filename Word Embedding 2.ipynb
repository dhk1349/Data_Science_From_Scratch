{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "#figsize(12, 8)\n",
    "\n",
    "from sklearn import svm\n",
    "from keras.utils import get_file\n",
    "import os\n",
    "import gensim\n",
    "import numpy as np\n",
    "import random\n",
    "import requests\n",
    "import geopandas as gpd\n",
    "from IPython.core.pylabtools import figsize\n",
    "import csv\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = 'GoogleNews-vectors-negative300.bin'\n",
    "unzipped = os.path.join('generated', MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isfile(unzipped):\n",
    "    with open(unzipped, 'wb') as fout:\n",
    "        zcat = subprocess.Popen(['zcat'],\n",
    "                          stdin=open(path),\n",
    "                          stdout=fout\n",
    "                         )\n",
    "        zcat.wait()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gensim.models.KeyedVectors.load_word2vec_format(unzipped, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('South_Korea', 0.8255740404129028),\n",
       " ('Korean', 0.7428451180458069),\n",
       " ('South_Korean', 0.6742696762084961),\n",
       " ('Seoul', 0.6671160459518433),\n",
       " ('Japan', 0.6590375304222107),\n",
       " ('Korea_ROK', 0.6261441111564636),\n",
       " ('Koreans', 0.62441086769104),\n",
       " ('Pool_KOREA_OUT', 0.6176227927207947),\n",
       " ('Tourism_Organization_KTO', 0.6147845983505249),\n",
       " ('SEOUL_NORTH', 0.6106680631637573)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(positive=['Korea'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "countries=pd.read_csv('countries.csv')\n",
    "positive=list(countries['name'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Since there are more than 3,000,000 words, it unlikey that there will be country names among randomly picked 5000words.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cleated',\n",
       " 'Lentigen_Corporation',\n",
       " 'Offeror_wholly_owned',\n",
       " 'jobboardtv@kwtx.com',\n",
       " 'Sukenick',\n",
       " 'birds_reptiles_amphibians',\n",
       " 'Houghten',\n",
       " 'www.facebook.com_IGT',\n",
       " 'GDNF_gene',\n",
       " 'Charles_Musyoki']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "negative=random.sample(model.vocab.keys(), 5000) \n",
    "negative[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Labelling Country and Non-country to 1 and 0 respectively</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Canada', 1),\n",
       " ('Turkmenistan', 1),\n",
       " ('Ethiopia', 1),\n",
       " ('Swaziland', 1),\n",
       " ('Czech_Republic', 1),\n",
       " ('Cameroon', 1),\n",
       " ('UAE', 1),\n",
       " ('Liberia', 1),\n",
       " ('Netherlands', 1),\n",
       " ('East_Timor', 1)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labelled=[(pos_word, 1) for pos_word in positive]+[(neg_word,0) for neg_word in negative]\n",
    "random.shuffle(labelled)\n",
    "labelled[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.13671875, -0.15429688,  0.26953125, ...,  0.02099609,\n",
       "         0.28515625, -0.2578125 ],\n",
       "       [-0.21875   ,  0.11035156,  0.02746582, ...,  0.265625  ,\n",
       "         0.23339844,  0.34765625],\n",
       "       [-0.02148438,  0.28125   ,  0.09619141, ..., -0.05517578,\n",
       "         0.11523438,  0.21582031],\n",
       "       ...,\n",
       "       [ 0.02893066,  0.07763672,  0.09228516, ..., -0.04785156,\n",
       "        -0.06030273,  0.17773438],\n",
       "       [ 0.03564453,  0.22070312,  0.16601562, ..., -0.04443359,\n",
       "        -0.38476562,  0.00052261],\n",
       "       [-0.0859375 ,  0.09228516, -0.06591797, ...,  0.09082031,\n",
       "        -0.15429688,  0.02148438]], dtype=float32)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x=np.asarray([model[w] for w,i in labelled])\n",
    "y=np.asarray([i for w,i in labelled])\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>70% of data will be used as training data</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_fraction=0.7 \n",
    "cut_off=int(training_fraction*len(labelled))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>With given data, separating sections of positive and negative data</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\n",
       "    decision_function_shape='ovr', degree=3, gamma='scale', kernel='linear',\n",
       "    max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "    tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf=svm.SVC(kernel='linear')\n",
    "clf.fit(x[:cut_off], y[:cut_off])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Making prediction on rest of 30% of data</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "result=clf.predict(x[cut_off:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "missed=[country for (pred, truth, country) in zip(result, y[cut_off:], labelled[cut_off:]) if pred!=truth]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(99.93573264781492, [('Venezuela', 0)])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "100-100*float(len(missed))/len(result), missed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\conda\\lib\\site-packages\\ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `syn0` (Attribute will be removed in 4.0.0, use self.vectors instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Shanghai',\n",
       " 'United_States',\n",
       " 'Thailand',\n",
       " 'Afghanistan',\n",
       " 'Kansas',\n",
       " 'UK',\n",
       " 'Iraqi',\n",
       " 'France',\n",
       " 'Idaho',\n",
       " 'Pennsylvania',\n",
       " 'Netherlands',\n",
       " 'Iowa',\n",
       " 'America',\n",
       " 'England',\n",
       " 'Queensland',\n",
       " 'Arkansas',\n",
       " 'Middle_East',\n",
       " 'Gaza',\n",
       " 'Tennessee',\n",
       " 'Dutch',\n",
       " 'North_Carolina',\n",
       " 'Sweden',\n",
       " 'Tehran',\n",
       " 'Texas',\n",
       " 'Taiwan',\n",
       " 'Nebraska',\n",
       " 'Sri_Lanka',\n",
       " 'Zimbabwe',\n",
       " 'Hong_Kong',\n",
       " 'Pakistan',\n",
       " 'Massachusetts',\n",
       " 'African',\n",
       " 'China',\n",
       " 'U.S.',\n",
       " 'Oklahoma',\n",
       " 'overseas',\n",
       " 'Spain',\n",
       " 'Alaska',\n",
       " 'Europe',\n",
       " 'Wales',\n",
       " 'India',\n",
       " 'Cuba',\n",
       " 'EU',\n",
       " 'Switzerland',\n",
       " 'Korea',\n",
       " 'Vermont',\n",
       " 'Bangladesh',\n",
       " 'Venezuela',\n",
       " 'Brazil',\n",
       " 'California']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_predictions = clf.predict(model.syn0)\n",
    "res=[]\n",
    "for w,prediction in zip(model.index2word, all_predictions):\n",
    "    if prediction:\n",
    "        res.append(w)\n",
    "        if len(res)==150:\n",
    "            break\n",
    "random.sample(res,50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Finding semantic distance among words</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "#country_to_idx={country['name']:idx for idx, country in enumerate(countries)}\n",
    "country_to_idx={}\n",
    "for idx in range(len(countries)):\n",
    "    country_to_idx[countries.at[idx,'name']]=idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_vecs=np.asarray([model[countries.at[c,'name']]for c in range(len(countries))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(184, 300)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "country_vecs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Canada 7.5440245\n",
      "New_Zealand 3.9619699\n",
      "Finland 3.9392405\n",
      "Puerto_Rico 3.838145\n",
      "Jamaica 3.8102934\n",
      "Sweden 3.8042789\n",
      "Slovakia 3.7038739\n",
      "Australia 3.6711009\n",
      "Bahamas 3.6240416\n",
      "United_States 3.5374336\n"
     ]
    }
   ],
   "source": [
    "dists=np.dot(country_vecs, country_vecs[country_to_idx['Canada']])\n",
    "for idx in reversed(np.argsort(dists)[-10:]):\n",
    "    print(countries.at[idx, 'name'], dists[idx])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
