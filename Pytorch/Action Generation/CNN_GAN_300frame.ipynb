{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchtext import data, datasets\n",
    "import dataloader as dl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "다음 기기로 학습합니다: cuda\n"
     ]
    }
   ],
   "source": [
    "# 하이퍼파라미터\n",
    "BATCH_SIZE = 64\n",
    "lr = 0.001\n",
    "EPOCHS = 500\n",
    "USE_CUDA = torch.cuda.is_available()\n",
    "#DEVICE = torch.device(\"cpu\")\n",
    "DEVICE = torch.device(\"cuda\" if USE_CUDA else \"cpu\")\n",
    "print(\"다음 기기로 학습합니다:\", DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader=dl.load_trainset(\"/home/dhk1349/Desktop/Capstone Design2/ntu/xsub/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Root directory for dataset\n",
    "dataroot = \"data/celeba\"\n",
    "\n",
    "# Number of workers for dataloader\n",
    "workers = 2\n",
    "\n",
    "# Batch size during training\n",
    "batch_size = 128\n",
    "\n",
    "# Spatial size of training images. All images will be resized to this\n",
    "#   size using a transformer.\n",
    "image_size = 64\n",
    "\n",
    "# Number of channels in the training images. For color images this is 3\n",
    "nc = 3\n",
    "\n",
    "# Size of z latent vector (i.e. size of generator input)\n",
    "nz = 100\n",
    "\n",
    "# Size of feature maps in generator\n",
    "ngf = 64\n",
    "\n",
    "# Size of feature maps in discriminator\n",
    "ndf = 64\n",
    "\n",
    "# Number of training epochs\n",
    "num_epochs = 5\n",
    "\n",
    "# Learning rate for optimizers\n",
    "lr = 0.0002\n",
    "\n",
    "# Beta1 hyperparam for Adam optimizers\n",
    "beta1 = 0.5\n",
    "\n",
    "# Number of GPUs available. Use 0 for CPU mode.\n",
    "ngpu = 1\n",
    "\n",
    "# Generator Code\n",
    "#nc - number of color channels in the input images. For color images this is 3\n",
    "#nz - length of latent vector (i.e. size of generator input)\n",
    "#ngf - relates to the depth of feature maps carried through the generator (Size of feature maps in generator)\n",
    "#ndf - sets the depth of feature maps propagated through the discriminator (Size of feature maps in discriminator)\n",
    "\n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        nn.init.constant_(m.bias.data, 0)\n",
    "\n",
    "nc=1 #channel이랄 것이 딱히 없음\n",
    "\n",
    "\n",
    "class DCGANGenerator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DCGANGenerator, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            # input is Z, going into a convolution\n",
    "            nn.ConvTranspose2d( nz, ngf * 4, 4, 1, 0, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 4),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ngf*8) x 4 x 4\n",
    "            nn.ConvTranspose2d(ngf * 4, ngf * 2, 4, (2,4), (1,0), bias=False),\n",
    "            nn.BatchNorm2d(ngf * 2),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ngf*4) x 8 x 16\n",
    "            \n",
    "            nn.ConvTranspose2d( ngf * 2, ngf, 4, 2, (2,1), bias=False, dilation =(1,2)),\n",
    "            nn.BatchNorm2d(ngf),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ngf*2) x 16 x 35\n",
    "            nn.ConvTranspose2d( ngf , 1, 4, 2, (1,0), bias=False, dilation=(1,2)),\n",
    "            nn.Tanh()\n",
    "            # state size. (ngf) x 32 x 75\n",
    "            \n",
    "        )\n",
    "    \n",
    "    def forward(self, input):\n",
    "        return self.main(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            #300*75\n",
    "            nn.Conv2d(1, ndf, 4, 2, 1, bias=False),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            \n",
    "            nn.Conv2d(ndf, ndf * 2, (8,4), 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 2),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            \n",
    "            nn.Conv2d(ndf * 2, ndf * 4, (8,4), 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 4),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            \n",
    "            nn.Conv2d(ndf * 4, ndf * 8, (8,4), (4,2), 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            \n",
    "            nn.Conv2d(ndf * 8, 1,(4, 4), (5,1), 0, bias=False),\n",
    "            \n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.main(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = Discriminator().to(DEVICE)\n",
    "G = DCGANGenerator().to(DEVICE)\n",
    "\n",
    "d_optimizer = torch.optim.Adam(D.parameters(), lr=lr)\n",
    "g_optimizer = torch.optim.Adam(G.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion=nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0/500], d_loss: 0.2956, g_loss: 7.5035, D(x): 0.74, D(G(z)): 0.00\n",
      "Epoch [1/500], d_loss: 0.5425, g_loss: 9.9828, D(x): 0.58, D(G(z)): 0.00\n",
      "Epoch [2/500], d_loss: 0.1098, g_loss: 8.0523, D(x): 0.90, D(G(z)): 0.00\n",
      "Epoch [3/500], d_loss: 0.0388, g_loss: 8.2849, D(x): 0.96, D(G(z)): 0.00\n",
      "Epoch [4/500], d_loss: 0.0111, g_loss: 7.8710, D(x): 0.99, D(G(z)): 0.00\n",
      "Epoch [5/500], d_loss: 0.0054, g_loss: 7.3326, D(x): 1.00, D(G(z)): 0.00\n",
      "Epoch [6/500], d_loss: 0.0017, g_loss: 9.7873, D(x): 1.00, D(G(z)): 0.00\n",
      "Epoch [7/500], d_loss: 1.0552, g_loss: 0.0000, D(x): 0.65, D(G(z)): 0.00\n",
      "Epoch [8/500], d_loss: 0.0000, g_loss: 0.0000, D(x): 0.00, D(G(z)): 0.00\n",
      "Epoch [9/500], d_loss: 0.0005, g_loss: 11.7993, D(x): 1.00, D(G(z)): 0.00\n",
      "Epoch [10/500], d_loss: 0.0000, g_loss: 12.4539, D(x): 1.00, D(G(z)): 0.00\n",
      "Epoch [11/500], d_loss: 0.0722, g_loss: 0.0000, D(x): 0.07, D(G(z)): 0.00\n",
      "Epoch [12/500], d_loss: 0.0000, g_loss: 0.0000, D(x): 0.00, D(G(z)): 0.00\n",
      "Epoch [13/500], d_loss: 0.0003, g_loss: 17.2066, D(x): 1.00, D(G(z)): 0.00\n",
      "Epoch [14/500], d_loss: 0.0004, g_loss: 0.0000, D(x): 0.00, D(G(z)): 0.00\n",
      "Epoch [15/500], d_loss: 0.0001, g_loss: 42.0510, D(x): 1.00, D(G(z)): 0.00\n",
      "Epoch [16/500], d_loss: 0.0000, g_loss: 39.3849, D(x): 1.00, D(G(z)): 0.00\n",
      "Epoch [17/500], d_loss: 0.0000, g_loss: 0.0000, D(x): 0.00, D(G(z)): 0.00\n",
      "Epoch [18/500], d_loss: 0.0000, g_loss: 0.0000, D(x): 0.00, D(G(z)): 0.00\n",
      "Epoch [19/500], d_loss: 0.0011, g_loss: 36.7524, D(x): 1.00, D(G(z)): 0.00\n",
      "Epoch [20/500], d_loss: 0.0000, g_loss: 0.0000, D(x): 0.00, D(G(z)): 0.00\n",
      "Epoch [21/500], d_loss: 0.0000, g_loss: 38.1838, D(x): 1.00, D(G(z)): 0.00\n",
      "Epoch [22/500], d_loss: 0.0000, g_loss: 0.0000, D(x): 0.00, D(G(z)): 0.00\n",
      "Epoch [23/500], d_loss: 0.0000, g_loss: 51.5599, D(x): 1.00, D(G(z)): 0.00\n",
      "Epoch [24/500], d_loss: 0.0000, g_loss: 0.0000, D(x): 0.00, D(G(z)): 0.00\n",
      "Epoch [25/500], d_loss: 0.0000, g_loss: 51.6225, D(x): 1.00, D(G(z)): 0.00\n",
      "Epoch [26/500], d_loss: 0.0303, g_loss: 4.8741, D(x): 1.00, D(G(z)): 0.03\n",
      "Epoch [27/500], d_loss: 0.0000, g_loss: 0.0000, D(x): 0.00, D(G(z)): 0.00\n",
      "Epoch [28/500], d_loss: 0.0115, g_loss: 50.8980, D(x): 0.99, D(G(z)): 0.00\n",
      "Epoch [29/500], d_loss: 0.0000, g_loss: 0.0000, D(x): 0.00, D(G(z)): 0.00\n",
      "Epoch [30/500], d_loss: 0.0000, g_loss: 0.0000, D(x): 0.00, D(G(z)): 0.00\n",
      "Epoch [31/500], d_loss: 0.0000, g_loss: 0.0000, D(x): 0.00, D(G(z)): 0.00\n",
      "Epoch [32/500], d_loss: 0.0000, g_loss: 0.0000, D(x): 0.00, D(G(z)): 0.00\n",
      "Epoch [33/500], d_loss: 0.0000, g_loss: 42.7387, D(x): 1.00, D(G(z)): 0.00\n",
      "Epoch [34/500], d_loss: 0.0000, g_loss: 50.5639, D(x): 1.00, D(G(z)): 0.00\n",
      "Epoch [35/500], d_loss: 0.0027, g_loss: 42.3910, D(x): 1.00, D(G(z)): 0.00\n",
      "Epoch [36/500], d_loss: 0.0000, g_loss: 57.0702, D(x): 1.00, D(G(z)): 0.00\n",
      "Epoch [37/500], d_loss: 0.0000, g_loss: 0.0000, D(x): 0.00, D(G(z)): 0.00\n",
      "Epoch [38/500], d_loss: 0.0000, g_loss: 0.0000, D(x): 0.00, D(G(z)): 0.00\n",
      "Epoch [39/500], d_loss: 0.0000, g_loss: 50.0783, D(x): 1.00, D(G(z)): 0.00\n",
      "Epoch [40/500], d_loss: 0.0000, g_loss: 56.1658, D(x): 1.00, D(G(z)): 0.00\n",
      "Epoch [41/500], d_loss: 0.0000, g_loss: 0.0000, D(x): 0.00, D(G(z)): 0.00\n",
      "Epoch [42/500], d_loss: 0.0000, g_loss: 55.8662, D(x): 1.00, D(G(z)): 0.00\n",
      "Epoch [43/500], d_loss: 0.0000, g_loss: 60.3164, D(x): 1.00, D(G(z)): 0.00\n",
      "Epoch [44/500], d_loss: 0.0000, g_loss: 0.0000, D(x): 0.00, D(G(z)): 0.00\n",
      "Epoch [45/500], d_loss: 0.0000, g_loss: 0.0000, D(x): 0.00, D(G(z)): 0.00\n",
      "Epoch [46/500], d_loss: 0.0000, g_loss: 0.0000, D(x): 0.00, D(G(z)): 0.00\n",
      "Epoch [47/500], d_loss: 0.0000, g_loss: 0.0000, D(x): 0.00, D(G(z)): 0.00\n",
      "Epoch [48/500], d_loss: 0.0000, g_loss: 0.0000, D(x): 0.00, D(G(z)): 0.00\n",
      "Epoch [49/500], d_loss: 0.0000, g_loss: 0.0000, D(x): 0.00, D(G(z)): 0.00\n",
      "Epoch [50/500], d_loss: 0.0000, g_loss: 59.2448, D(x): 1.00, D(G(z)): 0.00\n",
      "Epoch [51/500], d_loss: 0.0000, g_loss: 0.0000, D(x): 0.00, D(G(z)): 0.00\n",
      "Epoch [52/500], d_loss: 0.0000, g_loss: 60.0743, D(x): 1.00, D(G(z)): 0.00\n",
      "Epoch [53/500], d_loss: 0.0000, g_loss: 63.7464, D(x): 1.00, D(G(z)): 0.00\n",
      "Epoch [54/500], d_loss: 0.0000, g_loss: 0.0000, D(x): 0.00, D(G(z)): 0.00\n",
      "Epoch [55/500], d_loss: 0.0000, g_loss: 0.0000, D(x): 0.00, D(G(z)): 0.00\n",
      "Epoch [56/500], d_loss: 0.0000, g_loss: 59.6659, D(x): 1.00, D(G(z)): 0.00\n",
      "Epoch [57/500], d_loss: 0.0000, g_loss: 0.0000, D(x): 0.00, D(G(z)): 0.00\n",
      "Epoch [58/500], d_loss: 0.0000, g_loss: 0.0000, D(x): 0.00, D(G(z)): 0.00\n",
      "Epoch [59/500], d_loss: 0.0000, g_loss: 0.0000, D(x): 0.00, D(G(z)): 0.00\n",
      "Epoch [60/500], d_loss: 0.0000, g_loss: 0.0000, D(x): 0.00, D(G(z)): 0.00\n",
      "Epoch [61/500], d_loss: 0.0000, g_loss: 70.5833, D(x): 1.00, D(G(z)): 0.00\n",
      "Epoch [62/500], d_loss: 0.0000, g_loss: 55.5121, D(x): 1.00, D(G(z)): 0.00\n",
      "Epoch [63/500], d_loss: 0.0000, g_loss: 0.0000, D(x): 0.00, D(G(z)): 0.00\n",
      "Epoch [64/500], d_loss: 0.0000, g_loss: 0.0000, D(x): 0.00, D(G(z)): 0.00\n",
      "Epoch [65/500], d_loss: 0.0000, g_loss: 0.0000, D(x): 0.00, D(G(z)): 0.00\n",
      "Epoch [66/500], d_loss: 0.0000, g_loss: 0.0000, D(x): 0.00, D(G(z)): 0.00\n",
      "Epoch [67/500], d_loss: 0.0000, g_loss: 0.0000, D(x): 0.00, D(G(z)): 0.00\n",
      "Epoch [68/500], d_loss: 0.0000, g_loss: 61.8601, D(x): 1.00, D(G(z)): 0.00\n",
      "Epoch [69/500], d_loss: 0.0000, g_loss: 0.0000, D(x): 0.00, D(G(z)): 0.00\n",
      "Epoch [70/500], d_loss: 0.0000, g_loss: 0.0000, D(x): 0.00, D(G(z)): 0.00\n",
      "Epoch [71/500], d_loss: 0.0000, g_loss: 62.4476, D(x): 1.00, D(G(z)): 0.00\n",
      "Epoch [72/500], d_loss: 0.0000, g_loss: 72.1801, D(x): 1.00, D(G(z)): 0.00\n",
      "Epoch [73/500], d_loss: 0.0000, g_loss: 0.0000, D(x): 0.00, D(G(z)): 0.00\n",
      "Epoch [74/500], d_loss: 0.0000, g_loss: 0.0000, D(x): 0.00, D(G(z)): 0.00\n",
      "Epoch [75/500], d_loss: 0.0000, g_loss: 77.1769, D(x): 1.00, D(G(z)): 0.00\n",
      "Epoch [76/500], d_loss: 0.0000, g_loss: 70.7143, D(x): 1.00, D(G(z)): 0.00\n",
      "Epoch [77/500], d_loss: 0.0000, g_loss: 66.3532, D(x): 1.00, D(G(z)): 0.00\n",
      "Epoch [78/500], d_loss: 0.0000, g_loss: 0.0000, D(x): 0.00, D(G(z)): 0.00\n",
      "Epoch [79/500], d_loss: 0.0000, g_loss: 67.8471, D(x): 1.00, D(G(z)): 0.00\n",
      "Epoch [80/500], d_loss: 0.0000, g_loss: 0.0000, D(x): 0.00, D(G(z)): 0.00\n",
      "Epoch [81/500], d_loss: 0.0000, g_loss: 0.0000, D(x): 0.00, D(G(z)): 0.00\n",
      "Epoch [82/500], d_loss: 0.0000, g_loss: 0.0000, D(x): 0.00, D(G(z)): 0.00\n",
      "Epoch [83/500], d_loss: 0.0000, g_loss: 67.6643, D(x): 1.00, D(G(z)): 0.00\n",
      "Epoch [84/500], d_loss: 0.0000, g_loss: 71.3579, D(x): 1.00, D(G(z)): 0.00\n",
      "Epoch [85/500], d_loss: 0.0000, g_loss: 0.0000, D(x): 0.00, D(G(z)): 0.00\n",
      "Epoch [86/500], d_loss: 0.0000, g_loss: 62.4743, D(x): 1.00, D(G(z)): 0.00\n",
      "Epoch [87/500], d_loss: 0.0000, g_loss: 0.0000, D(x): 0.00, D(G(z)): 0.00\n",
      "Epoch [88/500], d_loss: 0.0000, g_loss: 64.0083, D(x): 1.00, D(G(z)): 0.00\n",
      "Epoch [89/500], d_loss: 0.0000, g_loss: 0.0000, D(x): 0.00, D(G(z)): 0.00\n",
      "Epoch [90/500], d_loss: 0.0000, g_loss: 74.9377, D(x): 1.00, D(G(z)): 0.00\n",
      "Epoch [91/500], d_loss: 0.0000, g_loss: 69.6822, D(x): 1.00, D(G(z)): 0.00\n",
      "Epoch [92/500], d_loss: 0.0000, g_loss: 61.8730, D(x): 1.00, D(G(z)): 0.00\n",
      "Epoch [93/500], d_loss: 0.0000, g_loss: 0.0000, D(x): 0.00, D(G(z)): 0.00\n",
      "Epoch [94/500], d_loss: 0.0000, g_loss: 68.0742, D(x): 1.00, D(G(z)): 0.00\n",
      "Epoch [95/500], d_loss: 0.0000, g_loss: 68.6971, D(x): 1.00, D(G(z)): 0.00\n",
      "Epoch [96/500], d_loss: 0.0000, g_loss: 76.0577, D(x): 1.00, D(G(z)): 0.00\n",
      "Epoch [97/500], d_loss: 0.0000, g_loss: 68.5180, D(x): 1.00, D(G(z)): 0.00\n",
      "Epoch [98/500], d_loss: 0.0000, g_loss: 0.0000, D(x): 0.00, D(G(z)): 0.00\n",
      "Epoch [99/500], d_loss: 0.0000, g_loss: 0.0000, D(x): 0.00, D(G(z)): 0.00\n",
      "Epoch [100/500], d_loss: 0.0000, g_loss: 68.4496, D(x): 1.00, D(G(z)): 0.00\n",
      "Epoch [101/500], d_loss: 0.0000, g_loss: 71.8468, D(x): 1.00, D(G(z)): 0.00\n",
      "Epoch [102/500], d_loss: 0.0000, g_loss: 0.0000, D(x): 0.00, D(G(z)): 0.00\n",
      "Epoch [103/500], d_loss: 0.0000, g_loss: 68.3419, D(x): 1.00, D(G(z)): 0.00\n",
      "Epoch [104/500], d_loss: 0.0000, g_loss: 0.0000, D(x): 0.00, D(G(z)): 0.00\n",
      "Epoch [105/500], d_loss: 0.0000, g_loss: 64.5287, D(x): 1.00, D(G(z)): 0.00\n",
      "Epoch [106/500], d_loss: 0.0000, g_loss: 0.0000, D(x): 0.00, D(G(z)): 0.00\n",
      "Epoch [107/500], d_loss: 0.0000, g_loss: 65.7630, D(x): 1.00, D(G(z)): 0.00\n",
      "Epoch [108/500], d_loss: 0.0000, g_loss: 0.0000, D(x): 0.00, D(G(z)): 0.00\n",
      "Epoch [109/500], d_loss: 0.0000, g_loss: 71.6513, D(x): 1.00, D(G(z)): 0.00\n",
      "Epoch [110/500], d_loss: 0.0000, g_loss: 0.0000, D(x): 0.00, D(G(z)): 0.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [111/500], d_loss: 0.0000, g_loss: 65.1557, D(x): 1.00, D(G(z)): 0.00\n",
      "Epoch [112/500], d_loss: 0.0000, g_loss: 0.0000, D(x): 0.00, D(G(z)): 0.00\n",
      "Epoch [113/500], d_loss: 0.0000, g_loss: 65.0711, D(x): 1.00, D(G(z)): 0.00\n",
      "Epoch [114/500], d_loss: 0.0000, g_loss: 0.0000, D(x): 0.00, D(G(z)): 0.00\n",
      "Epoch [115/500], d_loss: 0.0000, g_loss: 68.3027, D(x): 1.00, D(G(z)): 0.00\n",
      "Epoch [116/500], d_loss: 0.0000, g_loss: 73.6989, D(x): 1.00, D(G(z)): 0.00\n",
      "Epoch [117/500], d_loss: 0.0000, g_loss: 0.0000, D(x): 0.00, D(G(z)): 0.00\n",
      "Epoch [118/500], d_loss: 0.0000, g_loss: 0.0000, D(x): 0.00, D(G(z)): 0.00\n",
      "Epoch [119/500], d_loss: 0.0000, g_loss: 0.0000, D(x): 0.00, D(G(z)): 0.00\n",
      "Epoch [120/500], d_loss: 0.0000, g_loss: 68.7258, D(x): 1.00, D(G(z)): 0.00\n"
     ]
    }
   ],
   "source": [
    "total_step = len(train_loader)\n",
    "for epoch in range(EPOCHS):\n",
    "    for i, data in enumerate(train_loader):\n",
    "        x = np.array(data[0][0])\n",
    "        y=np.array(data[1])\n",
    "        \n",
    "        X=[]\n",
    "        \"\"\"\n",
    "        for i in range(30):\n",
    "            X.append(x[np.random.randint(i*10,(1+i)*10)].reshape(-1))\n",
    "        \"\"\"\n",
    "        X=np.array(x).reshape(1,300,75)\n",
    "        X=torch.tensor(np.array(X)).unsqueeze(0).to(DEVICE)\n",
    "        \n",
    "        #print(X.size())\n",
    "        \n",
    "        # '진짜'와 '가짜' 레이블 생성\n",
    "        real_labels = torch.tensor(y).to(torch.float32).to(DEVICE)\n",
    "        fake_labels = torch.tensor([0]).to(torch.float32).to(DEVICE)\n",
    "        \n",
    "        # 판별자가 진짜 이미지를 진짜로 인식하는 오차를 예산\n",
    "        outputs = D(X)[0][0][0]\n",
    "        \n",
    "        d_loss_real = criterion(outputs, real_labels)\n",
    "        real_score = outputs\n",
    "        #print(real_score)\n",
    "        # 무작위 텐서로 가짜 이미지 생성\n",
    "        #z = torch.randn(30,1)\n",
    "        #z = torch.tensor(np.array(z)).unsqueeze(0).to(DEVICE)\n",
    "        z=torch.randn(1, 100, 35, 1).to(DEVICE)\n",
    "\n",
    "        fake_images = torch.reshape(G(z),(1,1,300,75) )\n",
    "        \n",
    "        # 판별자가 가짜 이미지를 가짜로 인식하는 오차를 계산\n",
    "        outputs = D(fake_images)[0][0][0]\n",
    "        \n",
    "        d_loss_fake = criterion(outputs, fake_labels)\n",
    "        fake_score = outputs\n",
    "        \n",
    "        # 진짜와 가짜 이미지를 갖고 낸 오차를 더해서 판별자의 오차 계산\n",
    "        d_loss = d_loss_real + d_loss_fake\n",
    "\n",
    "        # 역전파 알고리즘으로 판별자 모델의 학습을 진행\n",
    "        d_optimizer.zero_grad()\n",
    "        g_optimizer.zero_grad()\n",
    "        d_loss.backward()\n",
    "        d_optimizer.step()\n",
    "        \n",
    "        # 생성자가 판별자를 속였는지에 대한 오차를 계산\n",
    "        fake_images = torch.reshape(G(z),(1,1,300,75) )\n",
    "        outputs = D(fake_images)[0][0][0]\n",
    "        g_loss = criterion(outputs, real_labels)\n",
    "        \n",
    "        # 역전파 알고리즘으로 생성자 모델의 학습을 진행\n",
    "        d_optimizer.zero_grad()\n",
    "        g_optimizer.zero_grad()\n",
    "        g_loss.backward()\n",
    "        g_optimizer.step()\n",
    "        \n",
    "    # 학습 진행 알아보기\n",
    "    print('Epoch [{}/{}], d_loss: {:.4f}, g_loss: {:.4f}, D(x): {:.2f}, D(G(z)): {:.2f}' \n",
    "          .format(epoch, EPOCHS, d_loss.item(), g_loss.item(), \n",
    "                  real_score.mean().item(), fake_score.mean().item()))\n",
    "    if(epoch%3==0):\n",
    "        torch.save(G.state_dict(), './snapshot/cnngan300/1106_cnn_'+str(epoch)+'.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_step = len(train_loader)\n",
    "EPOCH=500\n",
    "for epoch in range(100, 501):\n",
    "    for i, data in enumerate(train_loader):\n",
    "        x = np.array(data[0][0])\n",
    "        \n",
    "        X=[]\n",
    "        for i in range(30):\n",
    "            X.append(x[np.random.randint(i*10,(1+i)*10)].reshape(-1))\n",
    "        X=np.array(X).reshape(1,30,75)\n",
    "        X=torch.tensor(np.array(X)).unsqueeze(0).to(DEVICE)\n",
    "        \n",
    "        #print(X.size())\n",
    "        \n",
    "        # '진짜'와 '가짜' 레이블 생성\n",
    "        real_labels = torch.tensor([1]).to(torch.float32).to(DEVICE)\n",
    "        fake_labels = torch.tensor([1]).to(torch.float32).to(DEVICE)\n",
    "        \n",
    "        # 판별자가 진짜 이미지를 진짜로 인식하는 오차를 예산\n",
    "        outputs = D(X)[0][0][0]\n",
    "        \n",
    "        d_loss_real = criterion(outputs, real_labels)\n",
    "        real_score = outputs\n",
    "        #print(real_score)\n",
    "        # 무작위 텐서로 가짜 이미지 생성\n",
    "        #z = torch.randn(30,1)\n",
    "        #z = torch.tensor(np.array(z)).unsqueeze(0).to(DEVICE)\n",
    "        z=torch.randn(1, nz, 1, 1).to(DEVICE)\n",
    "        fake_images = torch.reshape(G(z),(1,1,30,75) )\n",
    "        \n",
    "        # 판별자가 가짜 이미지를 가짜로 인식하는 오차를 계산\n",
    "        outputs = D(fake_images)[0][0][0]\n",
    "        \n",
    "        d_loss_fake = criterion(outputs, fake_labels)\n",
    "        fake_score = outputs\n",
    "        \n",
    "        # 진짜와 가짜 이미지를 갖고 낸 오차를 더해서 판별자의 오차 계산\n",
    "        d_loss = d_loss_real + d_loss_fake\n",
    "        \n",
    "        # 역전파 알고리즘으로 판별자 모델의 학습을 진행\n",
    "        d_optimizer.zero_grad()\n",
    "        g_optimizer.zero_grad()\n",
    "        d_loss.backward()\n",
    "        d_optimizer.step()\n",
    "        \n",
    "        # 생성자가 판별자를 속였는지에 대한 오차를 계산\n",
    "        fake_images = torch.reshape(G(z),(1,1,30,75) )\n",
    "        outputs = D(fake_images)[0][0][0]\n",
    "        g_loss = criterion(outputs, real_labels)\n",
    "        \n",
    "        # 역전파 알고리즘으로 생성자 모델의 학습을 진행\n",
    "        d_optimizer.zero_grad()\n",
    "        g_optimizer.zero_grad()\n",
    "        g_loss.backward()\n",
    "        g_optimizer.step()\n",
    "        \n",
    "    # 학습 진행 알아보기\n",
    "    print('Epoch [{}/{}], d_loss: {:.4f}, g_loss: {:.4f}, D(x): {:.2f}, D(G(z)): {:.2f}' \n",
    "          .format(epoch, EPOCHS, d_loss.item(), g_loss.item(), \n",
    "                  real_score.mean().item(), fake_score.mean().item()))\n",
    "    if(epoch%3==0):\n",
    "        torch.save(G.state_dict(), './snapshot/1106_cnn_'+str(epoch)+'.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"./snapshot/gan_cnn_generated_action99.npy\", output.detach().numpy().reshape(1,30,75))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=1\n",
    "class test(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(test, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            #30*75로 해야겠음\n",
    "            # input is (nc) x 30 x 75\n",
    "            nn.Conv2d(a, ndf, 4, 2, 1, bias=False),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            \n",
    "            # state size. (ndf) x 15 x 37\n",
    "            nn.Conv2d(ndf, ndf * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 2),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            \n",
    "            # state size. (ndf*2) x 16 x 16\n",
    "            nn.Conv2d(ndf * 2, ndf * 4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 4),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            \n",
    "            # state size. (ndf*4) x 8 x 8\n",
    "            nn.Conv2d(ndf * 4, ndf * 8, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            \n",
    "            # state size. (ndf*8) x 4 x 4\n",
    "            nn.Conv2d(ndf * 8, 1,(1, 4), 1, 0, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.main(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_input=torch.randn(1, 1, 30, 75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testmodel=test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out=testmodel(test_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
