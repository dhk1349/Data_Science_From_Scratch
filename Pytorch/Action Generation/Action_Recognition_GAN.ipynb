{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchtext import data, datasets\n",
    "import dataloader as dl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "다음 기기로 학습합니다: cuda\n"
     ]
    }
   ],
   "source": [
    "# 하이퍼파라미터\n",
    "BATCH_SIZE = 64\n",
    "lr = 0.001\n",
    "EPOCHS = 10\n",
    "USE_CUDA = torch.cuda.is_available()\n",
    "#DEVICE = torch.device(\"cpu\")\n",
    "DEVICE = torch.device(\"cuda\" if USE_CUDA else \"cpu\")\n",
    "print(\"다음 기기로 학습합니다:\", DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader=dl.load_trainset()\n",
    "test_loader=dl.load_testset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    #def __init__(self, n_layers, hidden_dim, n_vocab, embed_dim, n_classes, dropout_p=0.2):\n",
    "    def __init__(self, n_layers, hidden_dim, input_shape, n_classes, dropout_p=0.2):\n",
    "        super(LSTM, self).__init__()\n",
    "        print(\"Building Basic LSTM model...\")\n",
    "        self.n_layers = n_layers \n",
    "        #self.embed = nn.Embedding(n_vocab, embed_dim)\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "        self.lstm = nn.LSTM(input_shape, self.hidden_dim,\n",
    "                          num_layers=self.n_layers,\n",
    "                          batch_first=True)\n",
    "        self.out = nn.Linear(self.hidden_dim, n_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        #x = self.embed(x)\n",
    "        h_0 = self._init_state(batch_size=x.size(0))\n",
    "        c_0 = self._init_state(batch_size=x.size(0))\n",
    "        x, _ = self.lstm(x, (h_0, c_0))  # [i, b, h]\n",
    "        h_t = x[:,-1,:]\n",
    "        self.dropout(h_t)\n",
    "        logit = self.out(h_t)  # [b, h] -> [b, o]\n",
    "        return logit\n",
    "    \n",
    "    def _init_state(self, batch_size=1):\n",
    "        weight = next(self.parameters()).data\n",
    "        return weight.new(self.n_layers, batch_size, self.hidden_dim).zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, train_iter):\n",
    "    model.train()\n",
    "    for batch in train_iter:\n",
    "        x, y = np.array(batch[0][0]), batch[1]\n",
    "        \n",
    "        X=[]\n",
    "        for i in range(10):\n",
    "            X.append(x[np.random.randint(i*10,(1+i)*10)].reshape(-1))\n",
    "        \n",
    "        X=torch.tensor(np.array(X)).unsqueeze(0).to(DEVICE)\n",
    "        y=y.to(DEVICE)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        logit = model(X)\n",
    "        loss = F.cross_entropy(logit, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \"\"\"\n",
    "def train(model, optimizer, data_idx):\n",
    "    model.train()\n",
    "    for idx, i in enumerate(data_idx): #suppose no batch\n",
    "        sample=data[i][...,0].transpose(1,0,2) #shape 300,3,25\n",
    "        #print(sample.shape)\n",
    "        \n",
    "        x=[] #Devide 300frame into 10 sectors and choose 1 frame from each sector\n",
    "        \n",
    "        for i in range(10):\n",
    "            x.append(sample[np.random.randint(i*10,(1+i)*10)].reshape(-1))\n",
    "        \n",
    "        X=torch.tensor(np.array(x)).unsqueeze(0).to(DEVICE)\n",
    "        y=torch.tensor(y_label[idx]).to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        logit = model(X)\n",
    "        loss = F.cross_entropy(logit, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \"\"\" \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, val_iter):\n",
    "    #evaluate model\n",
    "    model.eval()\n",
    "    corrects, total_loss = 0, 0\n",
    "    for batch in val_iter:\n",
    "        x, y = np.array(batch[0][0]), batch[1]\n",
    "        \n",
    "        X=[]\n",
    "        for i in range(10):\n",
    "            X.append(x[np.random.randint(i*10,(1+i)*10)].reshape(-1))\n",
    "        \n",
    "        X=torch.tensor(np.array(X)).unsqueeze(0).to(DEVICE)\n",
    "        y=y.to(DEVICE)\n",
    "        \n",
    "        logit = model(X)\n",
    "        loss = F.cross_entropy(logit, y, reduction='sum')\n",
    "        total_loss += loss.item()\n",
    "        corrects += (logit.max(1)[1].view(y.size()).data == y.data).sum()\n",
    "    size = len(val_iter.dataset)\n",
    "    avg_loss = total_loss / size\n",
    "    avg_accuracy = 100.0 * corrects / size\n",
    "    return avg_loss, avg_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \"\"\"\n",
    "def evaluate(model, val_iter):\n",
    "    #evaluate model\n",
    "    model.eval()\n",
    "    corrects, total_loss = 0, 0\n",
    "    for idx, i in enumerate(test_data_idx): #suppose no batch\n",
    "        sample=test_data[i][...,0].transpose(1,0,2) #shape 300,3,25\n",
    "        #print(sample.shape)\n",
    "        \n",
    "        x=[] #Devide 300frame into 10 sectors and choose 1 frame from each sector\n",
    "        \n",
    "        for i in range(10):\n",
    "            x.append(sample[np.random.randint(i*10,(1+i)*10)].reshape(-1))\n",
    "        \n",
    "        X=torch.tensor(np.array(x)).unsqueeze(0).to(DEVICE)\n",
    "        y=torch.tensor(test_y_label[idx]).to(DEVICE)\n",
    "    \n",
    "        logit = model(X)\n",
    "        loss = F.cross_entropy(logit, y, reduction='sum')\n",
    "        total_loss += loss.item()\n",
    "        corrects += (logit.max(1)[1].view(y.size()).data == y.data).sum()\n",
    "    size = len(val_iter.dataset)\n",
    "    avg_loss = total_loss / size\n",
    "    avg_accuracy = 100.0 * corrects / size\n",
    "    return avg_loss, avg_accuracy\n",
    "    \"\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LSTM(1, 256, 75, 2, 0.2).to(DEVICE)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_val_loss = None\n",
    "for e in range(1, EPOCHS+1):\n",
    "    train(model, optimizer, train_loader)\n",
    "    val_loss, val_accuracy = evaluate(model, test_loader)\n",
    "\n",
    "    print(\"[이폭: %d] 검증 오차:%5.2f | 검증 정확도:%5.2f\" % (e, val_loss, val_accuracy))\n",
    "    \n",
    "    # 검증 오차가 가장 적은 최적의 모델을 저장\n",
    "    if not best_val_loss or val_loss < best_val_loss:\n",
    "        if not os.path.isdir(\"snapshot\"):\n",
    "            os.makedirs(\"snapshot\")\n",
    "        torch.save(model.state_dict(), './snapshot/actionrecognition.pt')\n",
    "        best_val_loss = val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.load_state_dict(torch.load('./snapshot/txtclassification.pt'))\n",
    "test_loss, test_acc = evaluate(model, test_loader)\n",
    "print('테스트 오차: %5.2f | 테스트 정확도: %5.2f' % (test_loss, test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generator\n",
    "\n",
    "1 * 30-> 1 * 30 * 75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    #def __init__(self, n_layers, hidden_dim, n_vocab, embed_dim, n_classes, dropout_p=0.2):\n",
    "    def __init__(self, n_layers, hidden_dim, input_shape, n_classes, dropout_p=0.2):\n",
    "        super(Discriminator, self).__init__()\n",
    "        print(\"Building Basic Discriminator model...\")\n",
    "        self.n_layers = n_layers \n",
    "        #self.embed = nn.Embedding(n_vocab, embed_dim)\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "        self.lstm = nn.LSTM(input_shape, self.hidden_dim,\n",
    "                          num_layers=self.n_layers,\n",
    "                          batch_first=True)\n",
    "        self.fc = nn.Linear(self.hidden_dim, n_classes)\n",
    "        self.out = nn.Softmax()\n",
    "    def forward(self, x):\n",
    "        #x = self.embed(x)\n",
    "        h_0 = self._init_state(batch_size=x.size(0))\n",
    "        c_0 = self._init_state(batch_size=x.size(0))\n",
    "        x, _ = self.lstm(x, (h_0, c_0))  # [i, b, h]\n",
    "        h_t = x[:,-1,:]\n",
    "        self.dropout(h_t)\n",
    "        logit = self.fc(h_t)  # [b, h] -> [b, o]\n",
    "        logit = self.out(logit)\n",
    "        return logit\n",
    "    \n",
    "    def _init_state(self, batch_size=1):\n",
    "        weight = next(self.parameters()).data\n",
    "        return weight.new(self.n_layers, batch_size, self.hidden_dim).zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    #def __init__(self, n_layers, hidden_dim, n_vocab, embed_dim, n_classes, dropout_p=0.2):\n",
    "    def __init__(self, n_layers, hidden_dim, input_shape, dropout_p=0.2):\n",
    "        super(Generator, self).__init__()\n",
    "        print(\"Building Basic generator model...\")\n",
    "        self.n_layers = n_layers \n",
    "        #self.embed = nn.Embedding(n_vocab, embed_dim)\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "        self.lstm = nn.LSTM(input_shape, self.hidden_dim,\n",
    "                          num_layers=self.n_layers,\n",
    "                          batch_first=True)\n",
    "        #self.out = nn.Linear(self.hidden_dim, n_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        #x = self.embed(x)\n",
    "        h_0 = self._init_state(batch_size=x.size(0))\n",
    "        c_0 = self._init_state(batch_size=x.size(0))\n",
    "        x, (h, c) = self.lstm(x, (h_0, c_0))  # [i, b, h]\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def _init_state(self, batch_size=1):\n",
    "        weight = next(self.parameters()).data\n",
    "        return weight.new(self.n_layers, batch_size, self.hidden_dim).zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Basic Discriminator model...\n",
      "Building Basic generator model...\n"
     ]
    }
   ],
   "source": [
    "D = Discriminator(1, 256, 75, 2, 0.2).to(DEVICE)\n",
    "G = Generator(1, 75, 1, 0.2).to(DEVICE)\n",
    "\n",
    "d_optimizer = torch.optim.Adam(D.parameters(), lr=lr)\n",
    "g_optimizer = torch.optim.Adam(G.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion=nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data=torch.randn(30,1)\n",
    "input_data=torch.tensor(np.array(input_data)).unsqueeze(0).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader=dl.GAN_dataloader()\n",
    "EPOCHS=25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-3-82b9017cfe17>:23: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  logit = self.out(logit)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0/500], d_loss: 0.0172, g_loss: 6.8150, D(x): 0.50, D(G(z)): 0.50\n",
      "Epoch [1/500], d_loss: 0.0028, g_loss: 7.3366, D(x): 0.50, D(G(z)): 0.50\n",
      "Epoch [2/500], d_loss: 0.0034, g_loss: 5.9938, D(x): 0.50, D(G(z)): 0.50\n",
      "Epoch [3/500], d_loss: 0.0125, g_loss: 4.5129, D(x): 0.50, D(G(z)): 0.50\n",
      "Epoch [4/500], d_loss: 0.0016, g_loss: 7.2300, D(x): 0.50, D(G(z)): 0.50\n",
      "Epoch [5/500], d_loss: 0.0480, g_loss: 4.3716, D(x): 0.50, D(G(z)): 0.50\n",
      "Epoch [6/500], d_loss: 0.0180, g_loss: 4.0489, D(x): 0.50, D(G(z)): 0.50\n",
      "Epoch [7/500], d_loss: 0.0060, g_loss: 5.9748, D(x): 0.50, D(G(z)): 0.50\n",
      "Epoch [8/500], d_loss: 0.0005, g_loss: 7.6790, D(x): 0.50, D(G(z)): 0.50\n",
      "Epoch [9/500], d_loss: 0.0007, g_loss: 8.4321, D(x): 0.50, D(G(z)): 0.50\n",
      "Epoch [10/500], d_loss: 0.0101, g_loss: 6.0328, D(x): 0.50, D(G(z)): 0.50\n",
      "Epoch [11/500], d_loss: 0.0052, g_loss: 5.3993, D(x): 0.50, D(G(z)): 0.50\n",
      "Epoch [12/500], d_loss: 0.0011, g_loss: 6.8971, D(x): 0.50, D(G(z)): 0.50\n",
      "Epoch [13/500], d_loss: 0.0003, g_loss: 8.3151, D(x): 0.50, D(G(z)): 0.50\n",
      "Epoch [14/500], d_loss: 0.0007, g_loss: 7.6392, D(x): 0.50, D(G(z)): 0.50\n",
      "Epoch [15/500], d_loss: 0.0000, g_loss: 15.1906, D(x): 0.50, D(G(z)): 0.50\n",
      "Epoch [16/500], d_loss: 0.0004, g_loss: 7.9294, D(x): 0.50, D(G(z)): 0.50\n",
      "Epoch [17/500], d_loss: 0.0001, g_loss: 8.9420, D(x): 0.50, D(G(z)): 0.50\n",
      "Epoch [18/500], d_loss: 0.0002, g_loss: 8.3427, D(x): 0.50, D(G(z)): 0.50\n",
      "Epoch [19/500], d_loss: 0.0001, g_loss: 9.4221, D(x): 0.50, D(G(z)): 0.50\n",
      "Epoch [20/500], d_loss: 0.0000, g_loss: 10.5864, D(x): 0.50, D(G(z)): 0.50\n",
      "Epoch [21/500], d_loss: 0.0000, g_loss: 11.1651, D(x): 0.50, D(G(z)): 0.50\n",
      "Epoch [22/500], d_loss: 0.0000, g_loss: 10.5220, D(x): 0.50, D(G(z)): 0.50\n",
      "Epoch [23/500], d_loss: 0.0000, g_loss: 11.2315, D(x): 0.50, D(G(z)): 0.50\n",
      "Epoch [24/500], d_loss: 0.0000, g_loss: 11.6542, D(x): 0.50, D(G(z)): 0.50\n",
      "Epoch [25/500], d_loss: 0.0000, g_loss: 11.6801, D(x): 0.50, D(G(z)): 0.50\n",
      "Epoch [26/500], d_loss: 0.0000, g_loss: 12.3101, D(x): 0.50, D(G(z)): 0.50\n",
      "Epoch [27/500], d_loss: 0.0000, g_loss: 13.0076, D(x): 0.50, D(G(z)): 0.50\n",
      "Epoch [28/500], d_loss: 0.0000, g_loss: 13.5604, D(x): 0.50, D(G(z)): 0.50\n",
      "Epoch [29/500], d_loss: 0.0000, g_loss: 13.9951, D(x): 0.50, D(G(z)): 0.50\n",
      "Epoch [30/500], d_loss: 0.0000, g_loss: 14.1783, D(x): 0.50, D(G(z)): 0.50\n",
      "Epoch [31/500], d_loss: 0.0000, g_loss: 14.5551, D(x): 0.50, D(G(z)): 0.50\n",
      "Epoch [32/500], d_loss: 0.0000, g_loss: 12.0903, D(x): 0.50, D(G(z)): 0.50\n",
      "Epoch [33/500], d_loss: 0.0000, g_loss: 13.7704, D(x): 0.50, D(G(z)): 0.50\n",
      "Epoch [34/500], d_loss: 0.0000, g_loss: 16.2179, D(x): 0.50, D(G(z)): 0.50\n",
      "Epoch [35/500], d_loss: 0.0000, g_loss: 16.2356, D(x): 0.50, D(G(z)): 0.50\n",
      "Epoch [36/500], d_loss: 0.0000, g_loss: 16.2741, D(x): 0.50, D(G(z)): 0.50\n",
      "Epoch [37/500], d_loss: 0.0000, g_loss: 16.2105, D(x): 0.50, D(G(z)): 0.50\n",
      "Epoch [38/500], d_loss: 0.0000, g_loss: 16.1244, D(x): 0.50, D(G(z)): 0.50\n",
      "Epoch [39/500], d_loss: 0.0000, g_loss: 16.0780, D(x): 0.50, D(G(z)): 0.50\n",
      "Epoch [40/500], d_loss: 0.0000, g_loss: 16.2571, D(x): 0.50, D(G(z)): 0.50\n",
      "Epoch [41/500], d_loss: 0.0000, g_loss: 58.3232, D(x): 0.50, D(G(z)): 0.50\n",
      "Epoch [42/500], d_loss: 0.0000, g_loss: 58.5196, D(x): 0.50, D(G(z)): 0.50\n",
      "Epoch [43/500], d_loss: 0.0000, g_loss: 62.8650, D(x): 0.50, D(G(z)): 0.50\n",
      "Epoch [44/500], d_loss: 0.0000, g_loss: 60.1485, D(x): 0.50, D(G(z)): 0.50\n",
      "Epoch [45/500], d_loss: 0.0000, g_loss: 16.1311, D(x): 0.50, D(G(z)): 0.50\n",
      "Epoch [46/500], d_loss: 0.0000, g_loss: 16.2161, D(x): 0.50, D(G(z)): 0.50\n",
      "Epoch [47/500], d_loss: 0.0000, g_loss: 16.1460, D(x): 0.50, D(G(z)): 0.50\n",
      "Epoch [48/500], d_loss: 0.0000, g_loss: 16.2827, D(x): 0.50, D(G(z)): 0.50\n",
      "Epoch [49/500], d_loss: 0.0000, g_loss: 16.2691, D(x): 0.50, D(G(z)): 0.50\n",
      "Epoch [50/500], d_loss: 0.0000, g_loss: 13.7441, D(x): 0.50, D(G(z)): 0.50\n",
      "Epoch [51/500], d_loss: 0.0000, g_loss: 15.2428, D(x): 0.50, D(G(z)): 0.50\n",
      "Epoch [52/500], d_loss: 0.0000, g_loss: 15.8394, D(x): 0.50, D(G(z)): 0.50\n",
      "Epoch [53/500], d_loss: 0.0000, g_loss: 15.9856, D(x): 0.50, D(G(z)): 0.50\n",
      "Epoch [54/500], d_loss: 0.0000, g_loss: 16.2598, D(x): 0.50, D(G(z)): 0.50\n",
      "Epoch [55/500], d_loss: 0.0000, g_loss: 15.2827, D(x): 0.50, D(G(z)): 0.50\n",
      "Epoch [56/500], d_loss: 0.0000, g_loss: 15.8352, D(x): 0.50, D(G(z)): 0.50\n",
      "Epoch [57/500], d_loss: 0.0002, g_loss: 8.7935, D(x): 0.50, D(G(z)): 0.50\n",
      "Epoch [58/500], d_loss: 0.0000, g_loss: 13.0413, D(x): 0.50, D(G(z)): 0.50\n",
      "Epoch [59/500], d_loss: 0.0000, g_loss: 10.9878, D(x): 0.50, D(G(z)): 0.50\n",
      "Epoch [60/500], d_loss: 0.0000, g_loss: 11.1817, D(x): 0.50, D(G(z)): 0.50\n",
      "Epoch [61/500], d_loss: 0.0000, g_loss: 10.9992, D(x): 0.50, D(G(z)): 0.50\n",
      "Epoch [62/500], d_loss: 0.0001, g_loss: 9.4116, D(x): 0.50, D(G(z)): 0.50\n",
      "Epoch [63/500], d_loss: 0.0000, g_loss: 10.7775, D(x): 0.50, D(G(z)): 0.50\n",
      "Epoch [64/500], d_loss: 0.0000, g_loss: 11.8514, D(x): 0.50, D(G(z)): 0.50\n",
      "Epoch [65/500], d_loss: 0.0000, g_loss: 10.9728, D(x): 0.50, D(G(z)): 0.50\n",
      "Epoch [66/500], d_loss: 0.0000, g_loss: 12.8005, D(x): 0.50, D(G(z)): 0.50\n",
      "Epoch [67/500], d_loss: 0.0001, g_loss: 8.8896, D(x): 0.50, D(G(z)): 0.50\n",
      "Epoch [68/500], d_loss: 0.0000, g_loss: 14.1375, D(x): 0.50, D(G(z)): 0.50\n",
      "Epoch [69/500], d_loss: 0.0000, g_loss: 14.8693, D(x): 0.50, D(G(z)): 0.50\n",
      "Epoch [70/500], d_loss: 0.0000, g_loss: 15.2966, D(x): 0.50, D(G(z)): 0.50\n",
      "Epoch [71/500], d_loss: 0.0000, g_loss: 14.8207, D(x): 0.50, D(G(z)): 0.50\n",
      "Epoch [72/500], d_loss: 0.0000, g_loss: 15.1429, D(x): 0.50, D(G(z)): 0.50\n",
      "Epoch [73/500], d_loss: 0.0013, g_loss: 6.7061, D(x): 0.50, D(G(z)): 0.50\n",
      "Epoch [74/500], d_loss: 0.0030, g_loss: 6.1157, D(x): 0.50, D(G(z)): 0.50\n",
      "Epoch [75/500], d_loss: 0.0109, g_loss: 4.9762, D(x): 0.50, D(G(z)): 0.50\n",
      "Epoch [76/500], d_loss: 0.0025, g_loss: 6.0430, D(x): 0.50, D(G(z)): 0.50\n",
      "Epoch [77/500], d_loss: 0.0000, g_loss: 14.6194, D(x): 0.50, D(G(z)): 0.50\n",
      "Epoch [78/500], d_loss: 0.0002, g_loss: 8.7426, D(x): 0.50, D(G(z)): 0.50\n",
      "Epoch [79/500], d_loss: 0.0002, g_loss: 8.3685, D(x): 0.50, D(G(z)): 0.50\n",
      "Epoch [80/500], d_loss: 0.0000, g_loss: 12.7999, D(x): 0.50, D(G(z)): 0.50\n",
      "Epoch [81/500], d_loss: 0.0000, g_loss: 10.4614, D(x): 0.50, D(G(z)): 0.50\n",
      "Epoch [82/500], d_loss: 0.0001, g_loss: 9.8649, D(x): 0.50, D(G(z)): 0.50\n",
      "Epoch [83/500], d_loss: 0.0000, g_loss: 10.3083, D(x): 0.50, D(G(z)): 0.50\n",
      "Epoch [84/500], d_loss: 0.0001, g_loss: 9.7116, D(x): 0.50, D(G(z)): 0.50\n",
      "Epoch [85/500], d_loss: 0.0000, g_loss: 13.6590, D(x): 0.50, D(G(z)): 0.50\n",
      "Epoch [86/500], d_loss: 0.0001, g_loss: 9.1922, D(x): 0.50, D(G(z)): 0.50\n",
      "Epoch [87/500], d_loss: 0.0001, g_loss: 9.6147, D(x): 0.50, D(G(z)): 0.50\n",
      "Epoch [88/500], d_loss: 0.0000, g_loss: 10.3771, D(x): 0.50, D(G(z)): 0.50\n",
      "Epoch [89/500], d_loss: 0.0000, g_loss: 12.6127, D(x): 0.50, D(G(z)): 0.50\n",
      "Epoch [90/500], d_loss: 0.0000, g_loss: 14.1547, D(x): 0.50, D(G(z)): 0.50\n",
      "Epoch [91/500], d_loss: 0.0000, g_loss: 13.7198, D(x): 0.50, D(G(z)): 0.50\n",
      "Epoch [92/500], d_loss: 0.0000, g_loss: 14.7827, D(x): 0.50, D(G(z)): 0.50\n",
      "Epoch [93/500], d_loss: 0.0000, g_loss: 15.2059, D(x): 0.50, D(G(z)): 0.50\n",
      "Epoch [94/500], d_loss: 0.0000, g_loss: 16.0379, D(x): 0.50, D(G(z)): 0.50\n",
      "Epoch [95/500], d_loss: 0.0000, g_loss: 16.0491, D(x): 0.50, D(G(z)): 0.50\n",
      "Epoch [96/500], d_loss: 0.0000, g_loss: 16.0900, D(x): 0.50, D(G(z)): 0.50\n",
      "Epoch [97/500], d_loss: 0.0000, g_loss: 16.2333, D(x): 0.50, D(G(z)): 0.50\n",
      "Epoch [98/500], d_loss: 0.0000, g_loss: 16.1170, D(x): 0.50, D(G(z)): 0.50\n",
      "Epoch [99/500], d_loss: 0.0000, g_loss: 16.2223, D(x): 0.50, D(G(z)): 0.50\n",
      "Epoch [100/500], d_loss: 0.0000, g_loss: 16.2749, D(x): 0.50, D(G(z)): 0.50\n",
      "Epoch [101/500], d_loss: 0.0000, g_loss: 11.9201, D(x): 0.50, D(G(z)): 0.50\n",
      "Epoch [102/500], d_loss: 0.0000, g_loss: 12.4411, D(x): 0.50, D(G(z)): 0.50\n",
      "Epoch [103/500], d_loss: 0.0000, g_loss: 12.6788, D(x): 0.50, D(G(z)): 0.50\n",
      "Epoch [104/500], d_loss: 0.0000, g_loss: 12.6861, D(x): 0.50, D(G(z)): 0.50\n",
      "Epoch [105/500], d_loss: 0.0000, g_loss: 13.9651, D(x): 0.50, D(G(z)): 0.50\n",
      "Epoch [106/500], d_loss: 0.0000, g_loss: 13.8940, D(x): 0.50, D(G(z)): 0.50\n",
      "Epoch [107/500], d_loss: 0.0000, g_loss: 14.1197, D(x): 0.50, D(G(z)): 0.50\n",
      "Epoch [108/500], d_loss: 0.0000, g_loss: 13.1601, D(x): 0.50, D(G(z)): 0.50\n",
      "Epoch [109/500], d_loss: 0.0000, g_loss: 13.6157, D(x): 0.50, D(G(z)): 0.50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [110/500], d_loss: 0.0000, g_loss: 13.5409, D(x): 0.50, D(G(z)): 0.50\n",
      "Epoch [111/500], d_loss: 0.0000, g_loss: 13.3917, D(x): 0.50, D(G(z)): 0.50\n",
      "Epoch [112/500], d_loss: 0.0000, g_loss: 14.1644, D(x): 0.50, D(G(z)): 0.50\n",
      "Epoch [113/500], d_loss: 0.0000, g_loss: 14.5559, D(x): 0.50, D(G(z)): 0.50\n",
      "Epoch [114/500], d_loss: 0.0000, g_loss: 14.9056, D(x): 0.50, D(G(z)): 0.50\n",
      "Epoch [115/500], d_loss: 0.0000, g_loss: 13.8424, D(x): 0.50, D(G(z)): 0.50\n",
      "Epoch [116/500], d_loss: 0.0000, g_loss: 15.9921, D(x): 0.50, D(G(z)): 0.50\n",
      "Epoch [117/500], d_loss: 0.0000, g_loss: 16.1995, D(x): 0.50, D(G(z)): 0.50\n",
      "Epoch [118/500], d_loss: 0.0000, g_loss: 16.0746, D(x): 0.50, D(G(z)): 0.50\n",
      "Epoch [119/500], d_loss: 0.0000, g_loss: 69.8111, D(x): 0.50, D(G(z)): 0.50\n",
      "Epoch [120/500], d_loss: 0.0000, g_loss: 69.6978, D(x): 0.50, D(G(z)): 0.50\n",
      "Epoch [121/500], d_loss: 0.0000, g_loss: 69.8941, D(x): 0.50, D(G(z)): 0.50\n",
      "Epoch [122/500], d_loss: 0.0000, g_loss: 69.8372, D(x): 0.50, D(G(z)): 0.50\n",
      "Epoch [123/500], d_loss: 0.0000, g_loss: 69.8495, D(x): 0.50, D(G(z)): 0.50\n",
      "Epoch [124/500], d_loss: 0.0000, g_loss: 69.7985, D(x): 0.50, D(G(z)): 0.50\n",
      "Epoch [125/500], d_loss: 0.0000, g_loss: 69.7130, D(x): 0.50, D(G(z)): 0.50\n",
      "Epoch [126/500], d_loss: 0.0000, g_loss: 69.8243, D(x): 0.50, D(G(z)): 0.50\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-3aee797666ad>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0md_optimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0mg_optimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m         \u001b[0md_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m         \u001b[0md_optimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/torch/lib/python3.8/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m                 create_graph=create_graph)\n\u001b[0;32m--> 221\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/torch/lib/python3.8/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m    128\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m    131\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m         allow_unreachable=True)  # allow_unreachable flag\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "total_step = len(data_loader)\n",
    "for epoch in range(EPOCHS):\n",
    "    for i, data in enumerate(data_loader):\n",
    "        x = np.array(data[0][0])\n",
    "        \n",
    "        X=[]\n",
    "        for i in range(10):\n",
    "            X.append(x[np.random.randint(i*10,(1+i)*10)].reshape(-1))\n",
    "        \n",
    "        X=torch.tensor(np.array(X)).unsqueeze(0).to(DEVICE)\n",
    "        \n",
    "        # '진짜'와 '가짜' 레이블 생성\n",
    "        real_labels = torch.tensor([[0,1]]).to(torch.float32).to(DEVICE)\n",
    "        fake_labels = torch.tensor([[1,0]]).to(torch.float32).to(DEVICE)\n",
    "        \n",
    "        # 판별자가 진짜 이미지를 진짜로 인식하는 오차를 예산\n",
    "        outputs = D(X)\n",
    "        \n",
    "        d_loss_real = criterion(outputs, real_labels)\n",
    "        real_score = outputs\n",
    "        #print(real_score)\n",
    "        # 무작위 텐서로 가짜 이미지 생성\n",
    "        z = torch.randn(30,1)\n",
    "        z = torch.tensor(np.array(z)).unsqueeze(0).to(DEVICE)\n",
    "        fake_images = G(z)\n",
    "        \n",
    "        # 판별자가 가짜 이미지를 가짜로 인식하는 오차를 계산\n",
    "        outputs = D(fake_images)\n",
    "        d_loss_fake = criterion(outputs, fake_labels)\n",
    "        fake_score = outputs\n",
    "        \n",
    "        # 진짜와 가짜 이미지를 갖고 낸 오차를 더해서 판별자의 오차 계산\n",
    "        d_loss = d_loss_real + d_loss_fake\n",
    "\n",
    "        # 역전파 알고리즘으로 판별자 모델의 학습을 진행\n",
    "        d_optimizer.zero_grad()\n",
    "        g_optimizer.zero_grad()\n",
    "        d_loss.backward()\n",
    "        d_optimizer.step()\n",
    "        \n",
    "        # 생성자가 판별자를 속였는지에 대한 오차를 계산\n",
    "        fake_images = G(z)\n",
    "        outputs = D(fake_images)\n",
    "        g_loss = criterion(outputs, real_labels)\n",
    "        \n",
    "        # 역전파 알고리즘으로 생성자 모델의 학습을 진행\n",
    "        d_optimizer.zero_grad()\n",
    "        g_optimizer.zero_grad()\n",
    "        g_loss.backward()\n",
    "        g_optimizer.step()\n",
    "        \n",
    "    # 학습 진행 알아보기\n",
    "    print('Epoch [{}/{}], d_loss: {:.4f}, g_loss: {:.4f}, D(x): {:.2f}, D(G(z)): {:.2f}' \n",
    "          .format(epoch, EPOCHS, d_loss.item(), g_loss.item(), \n",
    "                  real_score.mean().item(), fake_score.mean().item()))\n",
    "    if(epoch%25==0):\n",
    "        torch.save(G.state_dict(), './snapshot/gan_'+str(epoch)+'.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
