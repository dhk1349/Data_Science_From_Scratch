{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Dataloader</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#skeleton code for dataset class\n",
    "class DiabetesDataset():\n",
    "    def __init__(self):\n",
    "        xy=np.loadtxt(\"D:\\\\4-2\\\\Machine Learning\\\\Pytorch\\\\data\\\\diabetes.csv.gz\", delimiter=',', dtype=np.float32)\n",
    "        self.x_data=Variable(torch.from_numpy(xy[:,0:-1]))\n",
    "        self.y_data=Variable(torch.from_numpy(xy[:, [-1]]))\n",
    "        self.len=xy.shape[0]\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return self.x_data[index], self.y_data[index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset=DiabetesDataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader=torch.utils.data.DataLoader(dataset=dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dhk13\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \"\"\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 1 | Intputs: torch.Size([32, 8]) | Labels: torch.Size([32, 1])\n",
      "EPOCH: 1 | Intputs: torch.Size([32, 8]) | Labels: torch.Size([32, 1])\n",
      "EPOCH: 1 | Intputs: torch.Size([32, 8]) | Labels: torch.Size([32, 1])\n",
      "EPOCH: 1 | Intputs: torch.Size([32, 8]) | Labels: torch.Size([32, 1])\n",
      "EPOCH: 1 | Intputs: torch.Size([32, 8]) | Labels: torch.Size([32, 1])\n",
      "EPOCH: 1 | Intputs: torch.Size([32, 8]) | Labels: torch.Size([32, 1])\n",
      "EPOCH: 1 | Intputs: torch.Size([32, 8]) | Labels: torch.Size([32, 1])\n",
      "EPOCH: 1 | Intputs: torch.Size([32, 8]) | Labels: torch.Size([32, 1])\n",
      "EPOCH: 1 | Intputs: torch.Size([32, 8]) | Labels: torch.Size([32, 1])\n",
      "EPOCH: 1 | Intputs: torch.Size([32, 8]) | Labels: torch.Size([32, 1])\n",
      "EPOCH: 1 | Intputs: torch.Size([32, 8]) | Labels: torch.Size([32, 1])\n",
      "EPOCH: 1 | Intputs: torch.Size([32, 8]) | Labels: torch.Size([32, 1])\n",
      "EPOCH: 1 | Intputs: torch.Size([32, 8]) | Labels: torch.Size([32, 1])\n",
      "EPOCH: 1 | Intputs: torch.Size([32, 8]) | Labels: torch.Size([32, 1])\n",
      "EPOCH: 1 | Intputs: torch.Size([32, 8]) | Labels: torch.Size([32, 1])\n",
      "EPOCH: 1 | Intputs: torch.Size([32, 8]) | Labels: torch.Size([32, 1])\n",
      "EPOCH: 1 | Intputs: torch.Size([32, 8]) | Labels: torch.Size([32, 1])\n",
      "EPOCH: 1 | Intputs: torch.Size([32, 8]) | Labels: torch.Size([32, 1])\n",
      "EPOCH: 1 | Intputs: torch.Size([32, 8]) | Labels: torch.Size([32, 1])\n",
      "EPOCH: 1 | Intputs: torch.Size([32, 8]) | Labels: torch.Size([32, 1])\n",
      "EPOCH: 1 | Intputs: torch.Size([32, 8]) | Labels: torch.Size([32, 1])\n",
      "EPOCH: 1 | Intputs: torch.Size([32, 8]) | Labels: torch.Size([32, 1])\n",
      "EPOCH: 1 | Intputs: torch.Size([32, 8]) | Labels: torch.Size([32, 1])\n",
      "EPOCH: 1 | Intputs: torch.Size([23, 8]) | Labels: torch.Size([23, 1])\n",
      "EPOCH: 2 | Intputs: torch.Size([32, 8]) | Labels: torch.Size([32, 1])\n",
      "EPOCH: 2 | Intputs: torch.Size([32, 8]) | Labels: torch.Size([32, 1])\n",
      "EPOCH: 2 | Intputs: torch.Size([32, 8]) | Labels: torch.Size([32, 1])\n",
      "EPOCH: 2 | Intputs: torch.Size([32, 8]) | Labels: torch.Size([32, 1])\n",
      "EPOCH: 2 | Intputs: torch.Size([32, 8]) | Labels: torch.Size([32, 1])\n",
      "EPOCH: 2 | Intputs: torch.Size([32, 8]) | Labels: torch.Size([32, 1])\n",
      "EPOCH: 2 | Intputs: torch.Size([32, 8]) | Labels: torch.Size([32, 1])\n",
      "EPOCH: 2 | Intputs: torch.Size([32, 8]) | Labels: torch.Size([32, 1])\n",
      "EPOCH: 2 | Intputs: torch.Size([32, 8]) | Labels: torch.Size([32, 1])\n",
      "EPOCH: 2 | Intputs: torch.Size([32, 8]) | Labels: torch.Size([32, 1])\n",
      "EPOCH: 2 | Intputs: torch.Size([32, 8]) | Labels: torch.Size([32, 1])\n",
      "EPOCH: 2 | Intputs: torch.Size([32, 8]) | Labels: torch.Size([32, 1])\n",
      "EPOCH: 2 | Intputs: torch.Size([32, 8]) | Labels: torch.Size([32, 1])\n",
      "EPOCH: 2 | Intputs: torch.Size([32, 8]) | Labels: torch.Size([32, 1])\n",
      "EPOCH: 2 | Intputs: torch.Size([32, 8]) | Labels: torch.Size([32, 1])\n",
      "EPOCH: 2 | Intputs: torch.Size([32, 8]) | Labels: torch.Size([32, 1])\n",
      "EPOCH: 2 | Intputs: torch.Size([32, 8]) | Labels: torch.Size([32, 1])\n",
      "EPOCH: 2 | Intputs: torch.Size([32, 8]) | Labels: torch.Size([32, 1])\n",
      "EPOCH: 2 | Intputs: torch.Size([32, 8]) | Labels: torch.Size([32, 1])\n",
      "EPOCH: 2 | Intputs: torch.Size([32, 8]) | Labels: torch.Size([32, 1])\n",
      "EPOCH: 2 | Intputs: torch.Size([32, 8]) | Labels: torch.Size([32, 1])\n",
      "EPOCH: 2 | Intputs: torch.Size([32, 8]) | Labels: torch.Size([32, 1])\n",
      "EPOCH: 2 | Intputs: torch.Size([32, 8]) | Labels: torch.Size([32, 1])\n",
      "EPOCH: 2 | Intputs: torch.Size([23, 8]) | Labels: torch.Size([23, 1])\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(2):\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        inputs, labels=data\n",
    "        \n",
    "        inputs, labels=torch.tensor(inputs), torch.tensor(labels)\n",
    "        \n",
    "        print(f'EPOCH: {epoch+1} | Intputs: {inputs.size()} | Labels: {labels.size()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Classifier for Diabetes</h1>\n",
    "<h3>3layers and sigmoid and final activation layer.</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.l1=torch.nn.Linear(8,6)\n",
    "        self.l2=torch.nn.Linear(6,4)\n",
    "        self.l3=torch.nn.Linear(4,1)\n",
    "        self.sigmoid=torch.nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out1=self.l1(x)\n",
    "        out2=self.l2(out1)\n",
    "        out3=self.l3(out2)\n",
    "        return self.sigmoid(out3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=Model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Loss: Binary entropy loss | Optimizer: Stochstic Gradient Descent</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion=torch.nn.BCELoss(reduction='sum')\n",
    "optimizer=torch.optim.SGD(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dhk13\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \"\"\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 1 | Batch: 1 | Loss: 21.6748\n",
      "EPOCH: 1 | Batch: 2 | Loss: 20.4761\n",
      "EPOCH: 1 | Batch: 3 | Loss: 20.2297\n",
      "EPOCH: 1 | Batch: 4 | Loss: 20.6045\n",
      "EPOCH: 1 | Batch: 5 | Loss: 19.8398\n",
      "EPOCH: 1 | Batch: 6 | Loss: 20.4376\n",
      "EPOCH: 1 | Batch: 7 | Loss: 19.7492\n",
      "EPOCH: 1 | Batch: 8 | Loss: 18.2205\n",
      "EPOCH: 1 | Batch: 9 | Loss: 22.0385\n",
      "EPOCH: 1 | Batch: 10 | Loss: 21.8989\n",
      "EPOCH: 1 | Batch: 11 | Loss: 19.7965\n",
      "EPOCH: 1 | Batch: 12 | Loss: 19.7615\n",
      "EPOCH: 1 | Batch: 13 | Loss: 19.5204\n",
      "EPOCH: 1 | Batch: 14 | Loss: 18.1345\n",
      "EPOCH: 1 | Batch: 15 | Loss: 21.8198\n",
      "EPOCH: 1 | Batch: 16 | Loss: 20.5075\n",
      "EPOCH: 1 | Batch: 17 | Loss: 18.1860\n",
      "EPOCH: 1 | Batch: 18 | Loss: 20.5557\n",
      "EPOCH: 1 | Batch: 19 | Loss: 18.2692\n",
      "EPOCH: 1 | Batch: 20 | Loss: 22.8052\n",
      "EPOCH: 1 | Batch: 21 | Loss: 17.0759\n",
      "EPOCH: 1 | Batch: 22 | Loss: 20.8041\n",
      "EPOCH: 1 | Batch: 23 | Loss: 17.9272\n",
      "EPOCH: 1 | Batch: 24 | Loss: 12.1342\n",
      "EPOCH: 2 | Batch: 1 | Loss: 19.2586\n",
      "EPOCH: 2 | Batch: 2 | Loss: 16.8793\n",
      "EPOCH: 2 | Batch: 3 | Loss: 20.1874\n",
      "EPOCH: 2 | Batch: 4 | Loss: 18.1259\n",
      "EPOCH: 2 | Batch: 5 | Loss: 16.5667\n",
      "EPOCH: 2 | Batch: 6 | Loss: 21.8824\n",
      "EPOCH: 2 | Batch: 7 | Loss: 15.5164\n",
      "EPOCH: 2 | Batch: 8 | Loss: 17.2876\n",
      "EPOCH: 2 | Batch: 9 | Loss: 17.9291\n",
      "EPOCH: 2 | Batch: 10 | Loss: 20.1720\n",
      "EPOCH: 2 | Batch: 11 | Loss: 18.9709\n",
      "EPOCH: 2 | Batch: 12 | Loss: 16.2636\n",
      "EPOCH: 2 | Batch: 13 | Loss: 17.4696\n",
      "EPOCH: 2 | Batch: 14 | Loss: 17.1593\n",
      "EPOCH: 2 | Batch: 15 | Loss: 12.4900\n",
      "EPOCH: 2 | Batch: 16 | Loss: 19.7237\n",
      "EPOCH: 2 | Batch: 17 | Loss: 19.2582\n",
      "EPOCH: 2 | Batch: 18 | Loss: 16.8130\n",
      "EPOCH: 2 | Batch: 19 | Loss: 15.9225\n",
      "EPOCH: 2 | Batch: 20 | Loss: 17.1780\n",
      "EPOCH: 2 | Batch: 21 | Loss: 13.5956\n",
      "EPOCH: 2 | Batch: 22 | Loss: 14.4687\n",
      "EPOCH: 2 | Batch: 23 | Loss: 19.8441\n",
      "EPOCH: 2 | Batch: 24 | Loss: 11.9796\n",
      "EPOCH: 3 | Batch: 1 | Loss: 14.9815\n",
      "EPOCH: 3 | Batch: 2 | Loss: 17.2367\n",
      "EPOCH: 3 | Batch: 3 | Loss: 13.8043\n",
      "EPOCH: 3 | Batch: 4 | Loss: 17.5491\n",
      "EPOCH: 3 | Batch: 5 | Loss: 16.9320\n",
      "EPOCH: 3 | Batch: 6 | Loss: 17.4807\n",
      "EPOCH: 3 | Batch: 7 | Loss: 13.7312\n",
      "EPOCH: 3 | Batch: 8 | Loss: 16.1936\n",
      "EPOCH: 3 | Batch: 9 | Loss: 18.5248\n",
      "EPOCH: 3 | Batch: 10 | Loss: 14.0233\n",
      "EPOCH: 3 | Batch: 11 | Loss: 13.9007\n",
      "EPOCH: 3 | Batch: 12 | Loss: 16.8633\n",
      "EPOCH: 3 | Batch: 13 | Loss: 17.8327\n",
      "EPOCH: 3 | Batch: 14 | Loss: 19.5572\n",
      "EPOCH: 3 | Batch: 15 | Loss: 15.3465\n",
      "EPOCH: 3 | Batch: 16 | Loss: 16.5026\n",
      "EPOCH: 3 | Batch: 17 | Loss: 12.8214\n",
      "EPOCH: 3 | Batch: 18 | Loss: 21.4010\n",
      "EPOCH: 3 | Batch: 19 | Loss: 15.3632\n",
      "EPOCH: 3 | Batch: 20 | Loss: 15.2272\n",
      "EPOCH: 3 | Batch: 21 | Loss: 17.2028\n",
      "EPOCH: 3 | Batch: 22 | Loss: 11.3053\n",
      "EPOCH: 3 | Batch: 23 | Loss: 17.1091\n",
      "EPOCH: 3 | Batch: 24 | Loss: 12.1345\n",
      "EPOCH: 4 | Batch: 1 | Loss: 12.9639\n",
      "EPOCH: 4 | Batch: 2 | Loss: 13.2772\n",
      "EPOCH: 4 | Batch: 3 | Loss: 15.1993\n",
      "EPOCH: 4 | Batch: 4 | Loss: 10.3248\n",
      "EPOCH: 4 | Batch: 5 | Loss: 13.1837\n",
      "EPOCH: 4 | Batch: 6 | Loss: 13.2694\n",
      "EPOCH: 4 | Batch: 7 | Loss: 19.0623\n",
      "EPOCH: 4 | Batch: 8 | Loss: 16.8256\n",
      "EPOCH: 4 | Batch: 9 | Loss: 14.7066\n",
      "EPOCH: 4 | Batch: 10 | Loss: 11.7528\n",
      "EPOCH: 4 | Batch: 11 | Loss: 21.8596\n",
      "EPOCH: 4 | Batch: 12 | Loss: 19.0242\n",
      "EPOCH: 4 | Batch: 13 | Loss: 14.6112\n",
      "EPOCH: 4 | Batch: 14 | Loss: 21.4333\n",
      "EPOCH: 4 | Batch: 15 | Loss: 14.4786\n",
      "EPOCH: 4 | Batch: 16 | Loss: 19.6438\n",
      "EPOCH: 4 | Batch: 17 | Loss: 19.2541\n",
      "EPOCH: 4 | Batch: 18 | Loss: 17.8718\n",
      "EPOCH: 4 | Batch: 19 | Loss: 9.4222\n",
      "EPOCH: 4 | Batch: 20 | Loss: 14.7736\n",
      "EPOCH: 4 | Batch: 21 | Loss: 12.6332\n",
      "EPOCH: 4 | Batch: 22 | Loss: 13.2911\n",
      "EPOCH: 4 | Batch: 23 | Loss: 17.0069\n",
      "EPOCH: 4 | Batch: 24 | Loss: 14.4300\n",
      "EPOCH: 5 | Batch: 1 | Loss: 13.9960\n",
      "EPOCH: 5 | Batch: 2 | Loss: 19.1323\n",
      "EPOCH: 5 | Batch: 3 | Loss: 14.0405\n",
      "EPOCH: 5 | Batch: 4 | Loss: 16.5016\n",
      "EPOCH: 5 | Batch: 5 | Loss: 14.5498\n",
      "EPOCH: 5 | Batch: 6 | Loss: 17.7543\n",
      "EPOCH: 5 | Batch: 7 | Loss: 12.4175\n",
      "EPOCH: 5 | Batch: 8 | Loss: 17.5549\n",
      "EPOCH: 5 | Batch: 9 | Loss: 16.7997\n",
      "EPOCH: 5 | Batch: 10 | Loss: 17.5252\n",
      "EPOCH: 5 | Batch: 11 | Loss: 20.2861\n",
      "EPOCH: 5 | Batch: 12 | Loss: 12.4428\n",
      "EPOCH: 5 | Batch: 13 | Loss: 16.5157\n",
      "EPOCH: 5 | Batch: 14 | Loss: 12.7150\n",
      "EPOCH: 5 | Batch: 15 | Loss: 12.4206\n",
      "EPOCH: 5 | Batch: 16 | Loss: 16.3525\n",
      "EPOCH: 5 | Batch: 17 | Loss: 19.1401\n",
      "EPOCH: 5 | Batch: 18 | Loss: 13.2381\n",
      "EPOCH: 5 | Batch: 19 | Loss: 17.8546\n",
      "EPOCH: 5 | Batch: 20 | Loss: 11.8382\n",
      "EPOCH: 5 | Batch: 21 | Loss: 18.1407\n",
      "EPOCH: 5 | Batch: 22 | Loss: 16.4882\n",
      "EPOCH: 5 | Batch: 23 | Loss: 13.6619\n",
      "EPOCH: 5 | Batch: 24 | Loss: 9.3834\n",
      "EPOCH: 6 | Batch: 1 | Loss: 13.5162\n",
      "EPOCH: 6 | Batch: 2 | Loss: 12.9048\n",
      "EPOCH: 6 | Batch: 3 | Loss: 17.4207\n",
      "EPOCH: 6 | Batch: 4 | Loss: 17.0074\n",
      "EPOCH: 6 | Batch: 5 | Loss: 16.3032\n",
      "EPOCH: 6 | Batch: 6 | Loss: 12.9584\n",
      "EPOCH: 6 | Batch: 7 | Loss: 13.0972\n",
      "EPOCH: 6 | Batch: 8 | Loss: 15.8478\n",
      "EPOCH: 6 | Batch: 9 | Loss: 16.3057\n",
      "EPOCH: 6 | Batch: 10 | Loss: 9.5788\n",
      "EPOCH: 6 | Batch: 11 | Loss: 13.2462\n",
      "EPOCH: 6 | Batch: 12 | Loss: 14.2914\n",
      "EPOCH: 6 | Batch: 13 | Loss: 18.5958\n",
      "EPOCH: 6 | Batch: 14 | Loss: 21.0918\n",
      "EPOCH: 6 | Batch: 15 | Loss: 13.9444\n",
      "EPOCH: 6 | Batch: 16 | Loss: 12.8095\n",
      "EPOCH: 6 | Batch: 17 | Loss: 15.3578\n",
      "EPOCH: 6 | Batch: 18 | Loss: 13.3229\n",
      "EPOCH: 6 | Batch: 19 | Loss: 18.6841\n",
      "EPOCH: 6 | Batch: 20 | Loss: 15.3130\n",
      "EPOCH: 6 | Batch: 21 | Loss: 20.3211\n",
      "EPOCH: 6 | Batch: 22 | Loss: 15.8508\n",
      "EPOCH: 6 | Batch: 23 | Loss: 15.3061\n",
      "EPOCH: 6 | Batch: 24 | Loss: 12.0769\n",
      "EPOCH: 7 | Batch: 1 | Loss: 14.1476\n",
      "EPOCH: 7 | Batch: 2 | Loss: 12.0404\n",
      "EPOCH: 7 | Batch: 3 | Loss: 16.9674\n",
      "EPOCH: 7 | Batch: 4 | Loss: 18.2170\n",
      "EPOCH: 7 | Batch: 5 | Loss: 19.2701\n",
      "EPOCH: 7 | Batch: 6 | Loss: 15.9347\n",
      "EPOCH: 7 | Batch: 7 | Loss: 18.6447\n",
      "EPOCH: 7 | Batch: 8 | Loss: 16.4755\n",
      "EPOCH: 7 | Batch: 9 | Loss: 13.0429\n",
      "EPOCH: 7 | Batch: 10 | Loss: 16.0619\n",
      "EPOCH: 7 | Batch: 11 | Loss: 13.1045\n",
      "EPOCH: 7 | Batch: 12 | Loss: 15.8269\n",
      "EPOCH: 7 | Batch: 13 | Loss: 12.4164\n",
      "EPOCH: 7 | Batch: 14 | Loss: 16.2084\n",
      "EPOCH: 7 | Batch: 15 | Loss: 12.2315\n",
      "EPOCH: 7 | Batch: 16 | Loss: 17.9913\n",
      "EPOCH: 7 | Batch: 17 | Loss: 15.4867\n",
      "EPOCH: 7 | Batch: 18 | Loss: 18.4399\n",
      "EPOCH: 7 | Batch: 19 | Loss: 14.1417\n",
      "EPOCH: 7 | Batch: 20 | Loss: 21.0608\n",
      "EPOCH: 7 | Batch: 21 | Loss: 19.9711\n",
      "EPOCH: 7 | Batch: 22 | Loss: 11.2475\n",
      "EPOCH: 7 | Batch: 23 | Loss: 14.5261\n",
      "EPOCH: 7 | Batch: 24 | Loss: 8.4512\n",
      "EPOCH: 8 | Batch: 1 | Loss: 15.1879\n",
      "EPOCH: 8 | Batch: 2 | Loss: 13.2679\n",
      "EPOCH: 8 | Batch: 3 | Loss: 14.4698\n",
      "EPOCH: 8 | Batch: 4 | Loss: 11.3922\n",
      "EPOCH: 8 | Batch: 5 | Loss: 15.3078\n",
      "EPOCH: 8 | Batch: 6 | Loss: 19.9821\n",
      "EPOCH: 8 | Batch: 7 | Loss: 10.4169\n",
      "EPOCH: 8 | Batch: 8 | Loss: 16.7814\n",
      "EPOCH: 8 | Batch: 9 | Loss: 14.8362\n",
      "EPOCH: 8 | Batch: 10 | Loss: 12.8703\n",
      "EPOCH: 8 | Batch: 11 | Loss: 20.3468\n",
      "EPOCH: 8 | Batch: 12 | Loss: 13.4040\n",
      "EPOCH: 8 | Batch: 13 | Loss: 19.0314\n",
      "EPOCH: 8 | Batch: 14 | Loss: 18.1706\n",
      "EPOCH: 8 | Batch: 15 | Loss: 17.4427\n",
      "EPOCH: 8 | Batch: 16 | Loss: 16.1826\n",
      "EPOCH: 8 | Batch: 17 | Loss: 18.9854\n",
      "EPOCH: 8 | Batch: 18 | Loss: 13.0170\n",
      "EPOCH: 8 | Batch: 19 | Loss: 14.5943\n",
      "EPOCH: 8 | Batch: 20 | Loss: 16.1113\n",
      "EPOCH: 8 | Batch: 21 | Loss: 16.3465\n",
      "EPOCH: 8 | Batch: 22 | Loss: 13.2615\n",
      "EPOCH: 8 | Batch: 23 | Loss: 15.2383\n",
      "EPOCH: 8 | Batch: 24 | Loss: 9.3644\n",
      "EPOCH: 9 | Batch: 1 | Loss: 16.7681\n",
      "EPOCH: 9 | Batch: 2 | Loss: 14.6715\n",
      "EPOCH: 9 | Batch: 3 | Loss: 13.5336\n",
      "EPOCH: 9 | Batch: 4 | Loss: 14.2358\n",
      "EPOCH: 9 | Batch: 5 | Loss: 21.4864\n",
      "EPOCH: 9 | Batch: 6 | Loss: 14.1091\n",
      "EPOCH: 9 | Batch: 7 | Loss: 13.1979\n",
      "EPOCH: 9 | Batch: 8 | Loss: 14.7525\n",
      "EPOCH: 9 | Batch: 9 | Loss: 14.5732\n",
      "EPOCH: 9 | Batch: 10 | Loss: 15.5027\n",
      "EPOCH: 9 | Batch: 11 | Loss: 13.2070\n",
      "EPOCH: 9 | Batch: 12 | Loss: 16.9762\n",
      "EPOCH: 9 | Batch: 13 | Loss: 13.6192\n",
      "EPOCH: 9 | Batch: 14 | Loss: 17.3680\n",
      "EPOCH: 9 | Batch: 15 | Loss: 19.5720\n",
      "EPOCH: 9 | Batch: 16 | Loss: 15.8327\n",
      "EPOCH: 9 | Batch: 17 | Loss: 9.6469\n",
      "EPOCH: 9 | Batch: 18 | Loss: 18.3802\n",
      "EPOCH: 9 | Batch: 19 | Loss: 14.0393\n",
      "EPOCH: 9 | Batch: 20 | Loss: 16.3811\n",
      "EPOCH: 9 | Batch: 21 | Loss: 15.3302\n",
      "EPOCH: 9 | Batch: 22 | Loss: 15.6153\n",
      "EPOCH: 9 | Batch: 23 | Loss: 17.9346\n",
      "EPOCH: 9 | Batch: 24 | Loss: 10.2308\n",
      "EPOCH: 10 | Batch: 1 | Loss: 17.9061\n",
      "EPOCH: 10 | Batch: 2 | Loss: 16.2646\n",
      "EPOCH: 10 | Batch: 3 | Loss: 14.9947\n",
      "EPOCH: 10 | Batch: 4 | Loss: 12.4247\n",
      "EPOCH: 10 | Batch: 5 | Loss: 10.7648\n",
      "EPOCH: 10 | Batch: 6 | Loss: 21.1166\n",
      "EPOCH: 10 | Batch: 7 | Loss: 18.3580\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 10 | Batch: 8 | Loss: 17.3943\n",
      "EPOCH: 10 | Batch: 9 | Loss: 17.4874\n",
      "EPOCH: 10 | Batch: 10 | Loss: 18.3511\n",
      "EPOCH: 10 | Batch: 11 | Loss: 11.4037\n",
      "EPOCH: 10 | Batch: 12 | Loss: 13.2522\n",
      "EPOCH: 10 | Batch: 13 | Loss: 16.1067\n",
      "EPOCH: 10 | Batch: 14 | Loss: 12.2385\n",
      "EPOCH: 10 | Batch: 15 | Loss: 21.1874\n",
      "EPOCH: 10 | Batch: 16 | Loss: 14.2830\n",
      "EPOCH: 10 | Batch: 17 | Loss: 13.0428\n",
      "EPOCH: 10 | Batch: 18 | Loss: 16.5961\n",
      "EPOCH: 10 | Batch: 19 | Loss: 14.1668\n",
      "EPOCH: 10 | Batch: 20 | Loss: 13.4899\n",
      "EPOCH: 10 | Batch: 21 | Loss: 16.9627\n",
      "EPOCH: 10 | Batch: 22 | Loss: 11.7492\n",
      "EPOCH: 10 | Batch: 23 | Loss: 15.7284\n",
      "EPOCH: 10 | Batch: 24 | Loss: 13.4898\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(10):\n",
    "     for i, data in enumerate(train_loader, 0):\n",
    "        inputs, labels=data\n",
    "        \n",
    "        inputs, labels=torch.tensor(inputs), torch.tensor(labels)\n",
    "        \n",
    "        y_pred=model(inputs)\n",
    "        \n",
    "        loss=criterion(y_pred, labels)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        print(f'EPOCH: {epoch+1} | Batch: {i+1} | Loss: {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Checking available dataset from torchvision</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CelebA, CIFAR, Cityscapes, COCO, Captions 등 다양한 dataset 사용이 가능하고, \n",
    "torchvision 라이브러리를 통해 사용할 수 있다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to D:\\4-2\\Machine Learning\\Pytorch\\data\\mnist\\MNIST\\raw\\train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66fb3eb51ced4dbf84b2801c9b2bbe31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracting D:\\4-2\\Machine Learning\\Pytorch\\data\\mnist\\MNIST\\raw\\train-images-idx3-ubyte.gz to D:\\4-2\\Machine Learning\\Pytorch\\data\\mnist\\MNIST\\raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to D:\\4-2\\Machine Learning\\Pytorch\\data\\mnist\\MNIST\\raw\\train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "751f8ed8a862472aaae05d99e0f6b523",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracting D:\\4-2\\Machine Learning\\Pytorch\\data\\mnist\\MNIST\\raw\\train-labels-idx1-ubyte.gz to D:\\4-2\\Machine Learning\\Pytorch\\data\\mnist\\MNIST\\raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to D:\\4-2\\Machine Learning\\Pytorch\\data\\mnist\\MNIST\\raw\\t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d9e359930af4adb9f3787ff113eff4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracting D:\\4-2\\Machine Learning\\Pytorch\\data\\mnist\\MNIST\\raw\\t10k-images-idx3-ubyte.gz to D:\\4-2\\Machine Learning\\Pytorch\\data\\mnist\\MNIST\\raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to D:\\4-2\\Machine Learning\\Pytorch\\data\\mnist\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6113211097794506ab86452596e7bb4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracting D:\\4-2\\Machine Learning\\Pytorch\\data\\mnist\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz to D:\\4-2\\Machine Learning\\Pytorch\\data\\mnist\\MNIST\\raw\n",
      "Processing...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "mnist_train = torchvision.datasets.MNIST('D:\\\\4-2\\\\Machine Learning\\\\Pytorch\\\\data\\\\mnist', train=True, download=True)\n",
    "data_loader = torch.utils.data.DataLoader(mnist_train,\n",
    "                                          batch_size=4,\n",
    "                                          shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training set/Validation set 중 어느 것을 선택하든, download option이 True이면 모든 데이터셋을 받아온다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Building Dataloader for titan dataset</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>A/5 21171</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17599</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C85</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Heikkinen, Miss. Laina</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>STON/O2. 3101282</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113803</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>C123</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Allen, Mr. William Henry</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>373450</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Survived  Pclass  \\\n",
       "0            1         0       3   \n",
       "1            2         1       1   \n",
       "2            3         1       3   \n",
       "3            4         1       1   \n",
       "4            5         0       3   \n",
       "\n",
       "                                                Name     Sex   Age  SibSp  \\\n",
       "0                            Braund, Mr. Owen Harris    male  22.0      1   \n",
       "1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
       "2                             Heikkinen, Miss. Laina  female  26.0      0   \n",
       "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n",
       "4                           Allen, Mr. William Henry    male  35.0      0   \n",
       "\n",
       "   Parch            Ticket     Fare Cabin Embarked  \n",
       "0      0         A/5 21171   7.2500   NaN        S  \n",
       "1      0          PC 17599  71.2833   C85        C  \n",
       "2      0  STON/O2. 3101282   7.9250   NaN        S  \n",
       "3      0            113803  53.1000  C123        S  \n",
       "4      0            373450   8.0500   NaN        S  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xy=pd.read_csv(\"D:\\\\4-2\\\\Machine Learning\\\\Pytorch\\\\data\\\\titanic\\\\train.csv\")\n",
    "xy.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data=Variable(torch.from_numpy(xy[:,0:-1]))\n",
    "y_data=Variable(torch.from_numpy(xy[:, [-1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#skeleton code for dataset class\n",
    "class Titanic():\n",
    "    def __init__(self):\n",
    "        xy=pd.read_csv(\"D:\\\\4-2\\\\Machine Learning\\\\Pytorch\\\\data\\\\titanic\\\\train.csv\")\n",
    "        \n",
    "        self.x_data=Variable(torch.from_numpy(xy[:,0:-1]))\n",
    "        self.y_data=Variable(torch.tensor(xy['Survived'].values))\n",
    "        self.len=xy.shape[0]\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return self.x_data[index], self.y_data[index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader=torch.utils.data.DataLoader(dataset=dataset, batch_size=32, shuffle=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
